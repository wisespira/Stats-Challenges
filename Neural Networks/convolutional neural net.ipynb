{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from torchvision import transforms  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashMNIST = keras.datasets.fashion_mnist\n",
    "(xTrainFull,yTrainFull),(xTest, yTest) = fashMNIST.load_data()\n",
    "xValidation, xTrain = xTrainFull[:5000]/255, xTrainFull[5000:]/255\n",
    "yValidation, yTrain = yTrainFull[:5000], yTrainFull[5000:]\n",
    "classNames = [\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\n",
    "              \"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain = xTrain.reshape(55000, 28, 28, 1)\n",
    "xValidation = xValidation.reshape(5000, 28, 28, 1)\n",
    "xTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(64,7,activation=\"relu\",padding=\"same\",input_shape=[28,28,1]),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(128,3,activation=\"relu\",padding=\"same\"),\n",
    "    keras.layers.Conv2D(128,3,activation=\"relu\",padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Conv2D(256,3,activation=\"relu\",padding=\"same\"),\n",
    "    keras.layers.Conv2D(256,3,activation=\"relu\",padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128,activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(64,activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10,activation=\"softmax\")    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"sgd\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\"fashion_CNN.h5\",save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 411s 239ms/step - loss: 0.6202 - accuracy: 0.7807 - val_loss: 0.4342 - val_accuracy: 0.8470\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 411s 239ms/step - loss: 0.5606 - accuracy: 0.7990 - val_loss: 0.4171 - val_accuracy: 0.8514\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 413s 241ms/step - loss: 0.5203 - accuracy: 0.8152 - val_loss: 0.3797 - val_accuracy: 0.8584\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 414s 241ms/step - loss: 0.4891 - accuracy: 0.8270 - val_loss: 0.3599 - val_accuracy: 0.8654\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 424s 247ms/step - loss: 0.4695 - accuracy: 0.8358 - val_loss: 0.3503 - val_accuracy: 0.8700\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 415s 242ms/step - loss: 0.4492 - accuracy: 0.8422 - val_loss: 0.3326 - val_accuracy: 0.8734\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 410s 239ms/step - loss: 0.4300 - accuracy: 0.8512 - val_loss: 0.3466 - val_accuracy: 0.8726\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 410s 238ms/step - loss: 0.4102 - accuracy: 0.8604 - val_loss: 0.3265 - val_accuracy: 0.8768\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 411s 239ms/step - loss: 0.3948 - accuracy: 0.8633 - val_loss: 0.3104 - val_accuracy: 0.8840\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 418s 243ms/step - loss: 0.3863 - accuracy: 0.8659 - val_loss: 0.3026 - val_accuracy: 0.8812\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 418s 243ms/step - loss: 0.3726 - accuracy: 0.8689 - val_loss: 0.2913 - val_accuracy: 0.8916\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 417s 242ms/step - loss: 0.3632 - accuracy: 0.8735 - val_loss: 0.2843 - val_accuracy: 0.8946\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 420s 244ms/step - loss: 0.3547 - accuracy: 0.8776 - val_loss: 0.2940 - val_accuracy: 0.8908\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 426s 248ms/step - loss: 0.3444 - accuracy: 0.8790 - val_loss: 0.3048 - val_accuracy: 0.8870\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 435s 253ms/step - loss: 0.3320 - accuracy: 0.8854 - val_loss: 0.2816 - val_accuracy: 0.8928\n",
      "Epoch 16/25\n",
      " 164/1719 [=>............................] - ETA: 5:57 - loss: 0.3409 - accuracy: 0.8841"
     ]
    }
   ],
   "source": [
    "history = model.fit(xTrain,yTrain,epochs=25,validation_data=(xValidation,yValidation), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"fashion_CNN.h5\") #rollback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.applications.resnet50.ResNet50(weights=\"imagenet\",include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "step must be greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-5ca2c4cf4fa7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimageResized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrans1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m244\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m244\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mimageResized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimageResized\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#y = model2.predict(inputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\resnet.py\u001b[0m in \u001b[0;36mpreprocess_input\u001b[1;34m(x, data_format)\u001b[0m\n\u001b[0;32m    518\u001b[0m               'keras.applications.resnet.preprocess_input')\n\u001b[0;32m    519\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m   return imagenet_utils.preprocess_input(\n\u001b[0m\u001b[0;32m    521\u001b[0m       x, data_format=data_format, mode='caffe')\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\imagenet_utils.py\u001b[0m in \u001b[0;36mpreprocess_input\u001b[1;34m(x, data_format, mode)\u001b[0m\n\u001b[0;32m    116\u001b[0m         x, data_format=data_format, mode=mode)\n\u001b[0;32m    117\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m     return _preprocess_symbolic_input(\n\u001b[0m\u001b[0;32m    119\u001b[0m         x, data_format=data_format, mode=mode)\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\imagenet_utils.py\u001b[0m in \u001b[0;36m_preprocess_symbolic_input\u001b[1;34m(x, data_format, mode)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m       \u001b[1;31m# 'RGB'->'BGR'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m103.939\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m116.779\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m123.68\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: step must be greater than zero"
     ]
    }
   ],
   "source": [
    "\n",
    "im = Image.open(\"C:/Users/Admin/Desktop/ART/data/Abstract_gallery/Abstract_image_51.jpg\")\n",
    "#im.show()\n",
    "trans = transforms.ToPILImage()\n",
    "trans1 = transforms.ToTensor()\n",
    "imageResized = trans1(im.resize((244,244)))\n",
    "imageResized.shape\n",
    "inputs = keras.applications.resnet50.preprocess_input(imageResized * 255)\n",
    "\n",
    "#y = model2.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
